<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ventures of Karsh</title>
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: #f4f4f9;
      margin: 0;
      padding: 0;
      color: #333;
    }

    header {
      background: linear-gradient(to right, #0f2027, #203a43, #2c5364);
      color: white;
      padding: 2rem 1rem;
      text-align: center;
    }

    header h1 {
      margin: 0;
      font-size: 2.5rem;
    }

    header p {
      font-size: 1.2rem;
      margin-top: 0.5rem;
    }

    section {
      max-width: 800px;
      margin: 2rem auto;
      padding: 0 1rem;
    }

    h2 {
      color: #2c5364;
      font-size: 1.8rem;
      margin-top: 2rem;
    }

    ul {
      padding-left: 1.5rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    blockquote {
      font-style: italic;
      background: #eef2f3;
      padding: 1rem;
      border-left: 4px solid #2c5364;
      margin: 2rem 0;
    }

    footer {
      text-align: center;
      padding: 1rem;
      font-size: 0.9rem;
      color: #777;
    }
  </style>
</head>
<body>
  <header>
    <h1>ventures_of_karsh</h1>
    <p>Learn to build, optimize, and interpret LLMs through the eyes of Karsh â€” a quiet AI apprentice on a hands-on journey through deep research and engineering.</p>
  </header>

  <section>
    <h2>ğŸ‘‹ Welcome to Karshâ€™s World of LLMs</h2>
    <p>Hey there â€” this is a space where practical engineering meets deep research in the world of Large Language Models (LLMs).</p>
    <p>But you wonâ€™t be alone here.</p>
    <p>Youâ€™ll be learning alongside <strong>Karsh</strong>, a shy but perseverant AI character â€” our fictional guide and metaphor for the inner researcher in all of us. In a world full of loud devs and flashy demos, Karsh quietly builds, breaks, optimizes, and rebuilds â€” chasing truth, not hype.</p>
    <p>This site is for LLM enthusiasts, researchers, and builders who want to go beyond surface-level prompting and really understand the systems behind todayâ€™s most powerful models.</p>

    <blockquote>
      â€œWhat I can't create, I don't understand.â€ â€” Richard Feynman
    </blockquote>

    <h2>ğŸ› ï¸ What to Expect</h2>
    <ul>
      <li>Hands-on engineering with open-source LLMs</li>
      <li>Optimization and deployment tricks (inference speed, memory, containerization)</li>
      <li>Guardrailing, redteaming, and safety evaluations</li>
      <li>Interpretable classification and explainable NLP</li>
      <li>Retrieval-Augmented Generation (RAG) and hallucination reduction</li>
      <li>Explorations in transformer ablations and next-gen architectures (SSMs, Mamba, etc.)</li>
    </ul>
    <p>All of this will be presented through technical posts, code walk-throughs, and research breakdowns â€” framed within Karshâ€™s fictional journey to make learning immersive, fun, and a bit narrative-driven.</p>

    <h2>ğŸ“š Why This Exists</h2>
    <p>Most LLM content out there is either too shallow or too academic. This space aims to strike a bridge between them â€” with a voice thatâ€™s technical, honest, and occasionally quirky.</p>
    <p>If youâ€™re someone who:</p>
    <ul>
      <li>Likes to build LLM apps and read SoTA papers</li>
      <li>Believes interpretability and optimization matter</li>
      <li>Wants to explore research topics hands-on, not just read them</li>
    </ul>
    <p>Then this journey is for you.</p>
    <p><strong>Follow along. Learn with Karsh. And letâ€™s build better AI systems â€” one curious experiment at a time.</strong></p>
  </section>

  <footer>
    &copy; 2025 Ventures of Karsh. Built with GitHub Pages.
  </footer>
</body>
</html>
